---
layout: single
title:  "pytorch"
categories: pytorch
tag: [pytorch, U-NET, tqdm]
---

# cuda

#### amp

Using amp in pytorch mostly mean that using **torch.cuda.amp.autocast** and **torch.cuda.amp.GradScale**.

They help the training time be lowered without affecting training performance -> Less GPU Usage, Better GPU Speed.



#### mixed precision

Using both datatype float16 and float32 to lower neural network's runtime and memory usages.

```python
import torch

scaler = torch.cuda.amp.autucast(enabled = True):
  outputs = model(inputs, targets)

loss = outputs["total_loss"]

opitimizer.zero_grads()
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```







------

# tqdm

```python
number_list = list(range(100))
for x in tqdm(number_list):
  sleep(0.05)
print("Completed!")
```

```python
def train_fn(loader, model, optimizer, loss_fn, scaler):
    loop = tqdm(loader)
    for batch_idx, (data, targets) in enumerate(loop):
        data = data.to(device=DEVICE)
```



![screenshot](../../images/2022-05-09-tqdm/screenshot.png)



