---
layout: single
title:  "pytorch"
categories: pytorch
---

# cuda

#### amp

Using amp in pytorch mostly mean that using **torch.cuda.amp.autocast** and **torch.cuda.amp.GradScale**.

They help the training time be lowered without affecting training performance -> Less GPU Usage, Better GPU Speed.



#### mixed precision

Using both datatype float16 and float32 to lower neural network's runtime and memory usages.

```python
import torch

scaler = torch.cuda.amp.autucast(enabled = True):
  outputs = model(inputs, targets)

loss = outputs["total_loss"]

opitimizer.zero_grads()
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```



------



![screenshot](../../images/2022-05-09-tqdm/screenshot.png)



--------------

